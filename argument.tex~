
Broadly, deepfake videos are real videos of speakers with facial and speech features realistically altered using deep learning neural networks. As of now, there is a conventional production pipeline for producing all deepfakes. First, the user must obtain a corpus of videos - preferably highly standardized with minimal stylistic variation - of the target actor. We hypothesize that politicians and news reporters are highly attractive candidates for selection, since they routinely produce standardized video content in the form of weekly addresses, press releases, campaign ads, and newsroom segments. Next, they must train the deep learning algorithm of choice in the identification of the speakers’ facial features. This is the most time- and resource-intensive step, requiring either hours on a GPU-enabled computing cluster or days to weeks of high-performance computation on a standard laptop. Additionally, they may choose to either train a text-to-speech deep learning model which learns to generate realistic voice samples of the speaker from input text or simply have an impersonator provide the voice inputs. Finally, either given footage of an impersonator’s facial features, a recorded voice performance, or some other input, the deep learning model can generate a video of synthetic facial movements which can be combined with audio to produce a “deepfake” of the target actor.

Three qualities in particular distinguish deepfakes from other contemporary forms of fake news. First is medium: deepfakes present information in the form of audiovisual stimuli, in contrast to textual stimuli. Like other forms of audiovisual political media such as political ads and news commentary (Mutz 2005; Ansolabehere and Iyengar 1997), deepfakes have the capacity to attach affective valence to political information in a way that textual fake news cannot. Second is expressed intent: deepfakes, thus far, have been produced by government actors, satirical entertainment news organizations, computer scientists developing deep learning technology, and  - in largest circulation - by “lone-wolf” unaffiliated media producers, predominantly on YouTube. Unlike much of recently circulated textual fake news which deceptively mimic the format, style, and source validity of sincere news media  (Nyhan, Guess 2018; Gentzkow 2018; Allcott, Gentzkow 2017), popular deepfakes are explicitly tagged as either satire, entertainment content, or technological demos by the producers themselves [cite examples]. Although many deepfakes have so far explicitly self-presented as entertainment or satirical media, others do not [Gabon president deepfake]. Moreover, experts warn that there is little stopping adversaries from using deepfakes for widespread deception. Finally, deepfakes  distort the speech and actions of a single target actor. Thus misinformation effects can be two-fold: (1) the viewer is misinformed that the actor actually made the lip-synched statement (2) the viewer is misinformed by the factual content of the actor’s statement. Just as the source of a textual fake news story might moderate information consumers’ responses, the particular target actor of a deepfake may similarly moderate a viewer’s response.

Taken altogether, a deepfake is distinctively characterized as a target actor lip-synching to what may be an arousing audiovisual performance generated by a media producer (e.g., Obama lip-synching to Jordan Peele calling President Trump a ‘complete and utter dipshit’), either sincerely labelled as a performance or insincerely guised as a news video. Given these characteristics, we hypothesize three different attitudinal effects of exposure to political deepfakes.

The first is deception of information, in that a viewer sincerely believes that the deepfake video depicts a real statement by the target actor. Deception, of course, is not unique to fake videos, however experts fear that if a video is convincingly photorealistic enough, there is little capacity for factual correction post-exposure. Consequently, deepfakes may mislead viewers about the target actors’ issue positions, intentions, judgments or beliefs (e.g., the viewer believes Obama is uncivil) or misinform viewers’ by the falsehood of the target actors’ manipulating statements (e.g., the viewer believes Obama when he says Trump is going to launch a nuclear attack).

The second is distrust in information. If a viewer is not deceived or they are exposed to an online video explicitly labelled as a doctored video, this exposure may still manifest in greater distrust towards all subsequently encountered political information, even from verified news sources. Prior work [cite Cristan Vaccari, this is my read of his APSA talk] suggests that exposure to one popularly circulated deepfake did not particularly result in informational deception, but rather confusion and consequent distrust in sincere news media. 

We propose a third attitudinal effect which is affective priming. Even if a deepfake neither engenders deception nor sows distrust, it can still very effectively mock, parody, or humiliate the target actor. Alternatively, it could depict the target actor mocking, parodying, or humiliate an out-partisan or an oppositional actor. Although the viewer may identify that the deepfake is a performance, and not reality, the video may still elicit an affective response that primes them to reframe the target actor either positively or negatively depending on whether the target actor is ex-ante favored or opposed and whether they are mocking or being mocked.
