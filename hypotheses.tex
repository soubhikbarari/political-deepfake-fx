Exposure to deepfakes may informationally deceive viewers, depress
their trust in all video media, or if, a particularly arousing
performance is generated, prime their attitudes towards particular
politicians. We present a few hypotheses on the relative magnitude of
these effects across important dimensions:

\begin{itemize}

\item[H$_1$:] The overall rate of deception from exposure to deepfakes
  will be low amongst all possible viewers. The viewerâ€™s digital
  literacy will negatively mediate their deception.

\item[H$_2$:] Reported distrust in information will be high for viewers
  who are able to identify that they are viewing a deepfake upon
  exposure.

\item[H$_3$:] People who recognize the candidate in the AARP video
  experiment will be less likely to be deceived by the deepfake itself

\item[H$_4$:] High political knowledge viewers in the AARP video
  experiment will be less likely to update their beliefs

\item[H$_5$:] Out-partisan viewers will more negatively evaluate a
  candidate after seeing them in the face transfer condition in the
  SNL experiment (but they will also negatively evaluate them in
  baseline)
\end{itemize}

To test these hypotheses, we conduct three experiments in a single
survey, fielded to 5,000 online survey respondents on Lucid. In the
first, we manipulate issue position. In the second, we test the affect
hypothesis. In the third, we test subjects ability to identify
deepfakes, when they are presented to them. In the next section, we
describe these three manipulations.

