Exposure to deepfakes may informationally deceive viewers, depress
their trust in all video media, or if, a particularly arousing
performance is generated, prime their attitudes towards particular
politicians. These effects are likely moderated by the subject's age
and digital literacy. We present a few hypotheses on the relative
magnitude of these effects across important dimensions:

\begin{itemize}

\item[H$_1$:] The overall rate of deception from exposure to deepfakes
  will be low amongst all possible viewers. The viewer's digital
  literacy will negatively moderate their deception.

\item[H$_2$:] Reported distrust in information will be high for viewers
  who are able to identify that they are viewing a deepfake upon
  exposure.

\item[H$_3$:] High political knowledge viewers in the informational
  video experiment will be less likely to update their beliefs and
  less likely to be deceived.

\item[H$_5$:] Out-partisan viewers will more negatively evaluate a
  candidate when a deepfake attempts to mock/ridicule them.
\end{itemize}

To test these hypotheses, we conduct three experiments in a single
survey, fielded to 5,000 online survey respondents on Lucid. In the
first experiment, we manipulate the issue position of candidates in
the Democratic Presidential Primaries. In the second, we test the
affect hypothesis through deepfakes that make fun of the candidate. In
the third, we test subjects' ability to identify deepfakes, as a test
of the deception hypothesis.

Subjects participate in \emph{either} experiment 1 (issue
manipulation) or experiment 2 (affect manipulation). \emph{All}
subjects participate in the identification experiment, only after
completing either experiment 1 or 2.

In the next section, we describe these three manipulations.

